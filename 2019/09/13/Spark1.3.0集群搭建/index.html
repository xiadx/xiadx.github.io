<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark," />










<meta name="description" content="Spark1.3.0集群搭建 CentOS 6.5集群搭建VirtualBox安装使用Virtual Box安装包，一步一步安装即可。Oracle_VM_VirtualBox_Extension_Pack-4.1.40-101594.vbox-extpack。之所以选用Virtual Box是因为它比VMWare更加稳定。使用VMWare运行hadoop集群或者spark集群时，有时会出现休眠后重">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark1.3.0集群搭建">
<meta property="og:url" content="http://example.com/2019/09/13/Spark1.3.0%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/index.html">
<meta property="og:site_name" content="Dingxin&#39;s BLOG">
<meta property="og:description" content="Spark1.3.0集群搭建 CentOS 6.5集群搭建VirtualBox安装使用Virtual Box安装包，一步一步安装即可。Oracle_VM_VirtualBox_Extension_Pack-4.1.40-101594.vbox-extpack。之所以选用Virtual Box是因为它比VMWare更加稳定。使用VMWare运行hadoop集群或者spark集群时，有时会出现休眠后重">
<meta property="og:locale">
<meta property="article:published_time" content="2019-09-13T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-13T07:12:57.728Z">
<meta property="article:author" content="Dingxin">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2019/09/13/Spark1.3.0集群搭建/"/>





  <title>Spark1.3.0集群搭建 | Dingxin's BLOG</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dingxin's BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/09/13/Spark1.3.0%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dingxin's BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark1.3.0集群搭建</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-13T17:00:00+08:00">
                2019-09-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Spark1.3.0集群搭建</p>
<h2 id="CentOS-6-5集群搭建"><a href="#CentOS-6-5集群搭建" class="headerlink" title="CentOS 6.5集群搭建"></a>CentOS 6.5集群搭建</h2><h3 id="VirtualBox安装"><a href="#VirtualBox安装" class="headerlink" title="VirtualBox安装"></a>VirtualBox安装</h3><p>使用Virtual Box安装包，一步一步安装即可。Oracle_VM_VirtualBox_Extension_Pack-4.1.40-101594.vbox-extpack。之所以选用Virtual Box是因为它比VMWare更加稳定。使用VMWare运行hadoop集群或者spark集群时，有时会出现休眠后重启时，某些进程莫名挂掉的问题。而Virtual Box没有这种情况。之所以选择Virtual Box 4.1版本，是因为更高的版本就不兼容win7了。<br>由于用的是Mac，所以自己安装VirturlBox。</p>
<p><a target="_blank" rel="noopener" href="https://www.virtualbox.org/">Virtual Box 官网</a></p>
<h3 id="CentOS-6-5安装"><a href="#CentOS-6-5安装" class="headerlink" title="CentOS 6.5安装"></a>CentOS 6.5安装</h3><p>使用CentOS 6.5镜像即可，CentOS-6.5-i386-minimal.iso。<br>创建虚拟机：打开Virtual Box，点击“新建”按钮，点击“下一步”，输入虚拟机名称为spark1，选择操作系统为Linux，选择版本为Red Hat，分配1024MB内存，后面的选项全部用默认，在Virtual Disk File location and size中，一定要自己选择一个目录来存放虚拟机文件，最后点击“create”按钮，开始创建虚拟机。<br>设置虚拟机网卡：选择创建好的spark1虚拟机，点击“设置”按钮，在网络一栏中，连接方式中，选择“Bridged Adapter”。<br>安装虚拟机中的CentOS 6.5操作系统：选择创建好的虚拟机spark1，点击“开始”按钮，选择安装介质（即本地的CentOS 6.5镜像文件），选择第一项开始安装-Skip-欢迎界面Next-选择默认语言-Baisc Storage Devices-Yes, discard any data-主机名:spark1-选择时区-设置初始密码为hadoop-Replace Existing Linux System-Write changes to disk-CentOS 6.5自己开始安装。<br>安装完以后，CentOS会提醒你要重启一下，就是reboot，你就reboot就可以了。</p>
<h3 id="CentOS-6-5网络配置"><a href="#CentOS-6-5网络配置" class="headerlink" title="CentOS 6.5网络配置"></a>CentOS 6.5网络配置</h3><p>先临时性设置虚拟机ip地址：ifconfig eth0 192.168.1.107，在&#x2F;etc&#x2F;hosts文件中配置本地ip（192.168.1.107）到host（spark1）的映射。<br>配置windows主机上的hosts文件：C:\Windows\System32\drivers\etc\hosts，192.168.1.107 spark1。<br>使用SecureCRT从windows上连接虚拟机，自己可以上网下一个SecureCRT的绿色版，网上很多。<br>永久性配置CentOS网络。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">DEVICE=eth0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=192.168.1.107</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.168.1.1</span><br></pre></td></tr></table></figure>

<p>重启网卡 service network restart。<br>即使更换了ip地址，重启网卡，可能还是联不通网。那么可以先将IPADDR、NETMASK、GATEWAY给删除，将BOOTPROTO改成dhcp。然后用service network restart重启网卡。此时linux会自动给分配一个ip地址，用ifconfig查看分配的ip地址。然后再次按照之前说的，配置网卡，将ip改成自动分配的ip地址。最后再重启一次网卡。</p>
<p>由于用的是Mac，所以自己安装SecureCRT，自己安装破解版。</p>
<p><a target="_blank" rel="noopener" href="https://www.vandyke.com/products/securecrt/">SecureCRT官网</a></p>
<h3 id="CentOS-6-5防火墙和DNS配置"><a href="#CentOS-6-5防火墙和DNS配置" class="headerlink" title="CentOS 6.5防火墙和DNS配置"></a>CentOS 6.5防火墙和DNS配置</h3><p>关闭防火墙</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure>
<p>自己在win7的控制面板中，关闭windows的防火墙！</p>
<p>配置dns服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/resolv.conf</span><br><span class="line">nameserver 61.139.2.69</span><br><span class="line">ping www.baidu.com</span><br></pre></td></tr></table></figure>

<h3 id="CentOS-6-5-yum配置"><a href="#CentOS-6-5-yum配置" class="headerlink" title="CentOS 6.5 yum配置"></a>CentOS 6.5 yum配置</h3><p>修改repo，使用WinSCP（网上很多，自己下一个），将CentOS6-Base-163.repo上传到CentOS中的&#x2F;usr&#x2F;local目录下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum.repos.d/</span><br><span class="line">rm -rf *</span><br></pre></td></tr></table></figure>
<p>将自己的repo文件移动到&#x2F;etc&#x2F;yum.repos.d&#x2F;目录中：cp &#x2F;usr&#x2F;local&#x2F;CentOS6-Base-163.repo .，修改repo文件，把所有gpgcheck属性修改为0。</p>
<p>配置yum</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line">yum install telnet</span><br></pre></td></tr></table></figure>

<h3 id="JDK-1-7安装"><a href="#JDK-1-7安装" class="headerlink" title="JDK 1.7安装"></a>JDK 1.7安装</h3><p>将jdk-7u60-linux-i586.rpm通过WinSCP上传到虚拟机中<br>安装JDK：rpm -ivh jdk-7u65-linux-i586.rpm<br>配置jdk相关的环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi .bashrc</span><br><span class="line">export JAVA_HOME=/usr/java/latest</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>
<p>测试jdk安装是否成功：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">rm -f /etc/udev/rules.d/70-persistent-net.rules</span><br></pre></td></tr></table></figure>
<p>由于用的是Mac，所以自己安装FileZilla，没有安装WinSCP。</p>
<h3 id="安装第二台和第三台虚拟机"><a href="#安装第二台和第三台虚拟机" class="headerlink" title="安装第二台和第三台虚拟机"></a>安装第二台和第三台虚拟机</h3><p>安装上述步骤，再安装两台一模一样环境的虚拟机，因为后面hadoop和spark都是要搭建集群的。<br>集群的最小环境就是三台。因为后面要搭建ZooKeeper、kafka等集群。<br>另外两台机器的hostname分别设置为spark2和spark3即可，ip分别为192.168.1.108和192.168.1.109<br>在安装的时候，另外两台虚拟机的centos镜像文件必须重新拷贝一份，放在新的目录里，使用各自自己的镜像文件。<br>虚拟机的硬盘文件也必须重新选择一个新的目录，以更好的区分。<br>安装好之后，记得要在三台机器的&#x2F;etc&#x2F;hosts文件中，配置全三台机器的ip地址到hostname的映射，而不能只配置本机，这个很重要！<br>在windows的hosts文件中也要配置全三台机器的ip地址到hostname的映射。</p>
<h3 id="配置集群ssh免密码登录"><a href="#配置集群ssh免密码登录" class="headerlink" title="配置集群ssh免密码登录"></a>配置集群ssh免密码登录</h3><p>首先在三台机器上配置对本机的ssh免密码登录。生成本机的公钥，过程中不断敲回车即可，ssh-keygen命令默认会将公钥放在&#x2F;root&#x2F;.ssh目录下。ssh-keygen -t rsa。将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /root/.ssh</span><br><span class="line">cp id_rsa.pub authorized_keys</span><br></pre></td></tr></table></figure>

<p>接着配置三台机器互相之间的ssh免密码登录。使用ssh-copy-id -i spark命令将本机的公钥拷贝到指定机器的authorized_keys文件中（方便好用）。</p>
<h2 id="Hadoop-2-4-1集群搭建"><a href="#Hadoop-2-4-1集群搭建" class="headerlink" title="Hadoop 2.4.1集群搭建"></a>Hadoop 2.4.1集群搭建</h2><h3 id="安装hadoop包"><a href="#安装hadoop包" class="headerlink" title="安装hadoop包"></a>安装hadoop包</h3><p>使用hadoop-2.4.1.tar.gz，使用WinSCP上传到CentOS的&#x2F;usr&#x2F;local目录下<br>将hadoop包进行解压缩：tar -zxvf hadoop-2.4.1.tar.gz<br>对hadoop目录进行重命名：mv hadoop-2.4.1 hadoop<br>配置hadoop相关环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi .bashrc</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>

<p>由于用的是Mac，所以自己使用FileZilla，不是WinSCP。</p>
<h3 id="修改core-site-xml"><a href="#修改core-site-xml" class="headerlink" title="修改core-site.xml"></a>修改core-site.xml</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://spark1:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="修改hdfs-site-xml"><a href="#修改hdfs-site-xml" class="headerlink" title="修改hdfs-site.xml"></a>修改hdfs-site.xml</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/data/namenode&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/data/datanode&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/local/data/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">修改mapred-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">修改yarn-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;spark1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="修改slaves文件"><a href="#修改slaves文件" class="headerlink" title="修改slaves文件"></a>修改slaves文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark1</span><br><span class="line">spark2</span><br><span class="line">spark3</span><br></pre></td></tr></table></figure>
<h3 id="在另外两台机器上搭建hadoop"><a href="#在另外两台机器上搭建hadoop" class="headerlink" title="在另外两台机器上搭建hadoop"></a>在另外两台机器上搭建hadoop</h3><p>使用如上配置在另外两台机器上搭建hadoop，可以使用scp命令将spark1上面的hadoop安装包和.bashrc配置文件都拷贝过去。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/local/hadoop root@spark2:/usr/local/</span><br><span class="line">scp -r /usr/local/hadoop root@spark3:/usr/local/</span><br><span class="line">scp .bashrc root@spark2:~/</span><br><span class="line">scp .bashrc root@spark3:~/</span><br></pre></td></tr></table></figure>
<p>要记得对.bashrc文件进行source，以让它生效。<br>记得在spark2和spark3的&#x2F;usr&#x2F;local目录下创建data目录。</p>
<h3 id="启动hdfs集群"><a href="#启动hdfs集群" class="headerlink" title="启动hdfs集群"></a>启动hdfs集群</h3><p>格式化namenode：在spark1上执行以下命令，<code>hdfs namenode -format</code><br>启动hdfs集群：<code>start-dfs.sh</code><br> 验证启动是否成功：jps、访问spark:50070 (关闭CentOS 6.5 防火墙)。spark1：namenode、datanode、secondarynamenode。spark2：datanode。spark3：datanode。</p>
<h3 id="启动yarn集群"><a href="#启动yarn集群" class="headerlink" title="启动yarn集群"></a>启动yarn集群</h3><p>启动yarn集群：<code>start-yarn.sh</code><br> 验证启动是否成功：jps、访问spark:8088(关闭CentOS 6.5 防火墙)。spark1：resourcemanager、nodemanager。spark2：nodemanager。spark3：nodemanager。</p>
<h3 id="Hive-0-13搭建"><a href="#Hive-0-13搭建" class="headerlink" title="Hive 0.13搭建"></a>Hive 0.13搭建</h3><p>安装hive包<br>将课程提供的apache-hive-0.13.1-bin.tar.gz使用WinSCP上传到spark1的&#x2F;usr&#x2F;local目录下。<br>解压缩hive安装包：tar -zxvf apache-hive-0.13.1-bin.tar.gz。<br>重命名hive目录：mv apache-hive-0.13.1-bin hive。<br>配置hive相关的环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi .bashrc</span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$HIVE_HOME/bin</span><br><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>
<h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><p>在spark1上安装mysql<br>使用yum安装mysql server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y mysql-server</span><br><span class="line">service mysqld start</span><br><span class="line">chkconfig mysqld on</span><br></pre></td></tr></table></figure>
<p>使用yum安装mysql connector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y mysql-connector-java</span><br></pre></td></tr></table></figure>
<p>将mysql connector拷贝到hive的lib包中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib</span><br></pre></td></tr></table></figure>
<p>在mysql上创建hive元数据库，并对hive进行授权</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists hive_metadata;</span><br><span class="line">grant all privileges on hive_metadata.* to &#x27;hive&#x27;@&#x27;%&#x27; identified by &#x27;hive&#x27;;</span><br><span class="line">grant all privileges on hive_metadata.* to &#x27;hive&#x27;@&#x27;localhost&#x27; identified by &#x27;hive&#x27;;</span><br><span class="line">grant all privileges on hive_metadata.* to &#x27;hive&#x27;@&#x27;spark1&#x27; identified by &#x27;hive&#x27;;</span><br><span class="line">flush privileges;</span><br><span class="line">use hive_metadata;</span><br></pre></td></tr></table></figure>
<h3 id="配置hive-site-xml"><a href="#配置hive-site-xml" class="headerlink" title="配置hive-site.xml"></a>配置hive-site.xml</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">mv hive-default.xml.template hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql://spark1:3306/hive_metadata?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="配置hive-env-sh和hive-config-sh"><a href="#配置hive-env-sh和hive-config-sh" class="headerlink" title="配置hive-env.sh和hive-config.sh"></a>配置hive-env.sh和hive-config.sh</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mv hive-env.sh.template hive-env.sh</span><br><span class="line">vi /usr/local/hive/bin/hive-config.sh</span><br><span class="line">export JAVA_HOME=/usr/java/latest</span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure>
<h3 id="验证hive是否安装成功"><a href="#验证hive是否安装成功" class="headerlink" title="验证hive是否安装成功"></a>验证hive是否安装成功</h3><p>直接输入hive命令，可以进入hive命令行</p>
<h2 id="ZooKeeper-3-4-5集群搭建"><a href="#ZooKeeper-3-4-5集群搭建" class="headerlink" title="ZooKeeper 3.4.5集群搭建"></a>ZooKeeper 3.4.5集群搭建</h2><h3 id="安装ZooKeeper包"><a href="#安装ZooKeeper包" class="headerlink" title="安装ZooKeeper包"></a>安装ZooKeeper包</h3><p>将zookeeper-3.4.5.tar.gz使用WinSCP拷贝到spark1的&#x2F;usr&#x2F;local目录下。<br>对zookeeper-3.4.5.tar.gz进行解压缩：tar -zxvf zookeeper-3.4.5.tar.gz。<br>对zookeeper目录进行重命名：mv zookeeper-3.4.5 zk。<br>配置zookeeper相关的环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi .bashrc</span><br><span class="line">export ZOOKEEPER_HOME=/usr/local/zk</span><br><span class="line">export PATH=$ZOOKEEPER_HOME/bin</span><br><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>
<h3 id="配置zoo-cfg"><a href="#配置zoo-cfg" class="headerlink" title="配置zoo.cfg"></a>配置zoo.cfg</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd zk/conf</span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line">vi zoo.cfg</span><br><span class="line">dataDir=/usr/local/zk/data</span><br><span class="line">server.0=spark1:2888:3888	</span><br><span class="line">server.1=spark2:2888:3888</span><br><span class="line">server.2=spark3:2888:3888</span><br></pre></td></tr></table></figure>
<h3 id="设置zk节点标识"><a href="#设置zk节点标识" class="headerlink" title="设置zk节点标识"></a>设置zk节点标识</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd zk</span><br><span class="line">mkdir data</span><br><span class="line">cd data</span><br><span class="line">vi myid</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<h3 id="搭建zk集群"><a href="#搭建zk集群" class="headerlink" title="搭建zk集群"></a>搭建zk集群</h3><p>在另外两个节点上按照上述步骤配置ZooKeeper，使用scp将zk和.bashrc拷贝到spark2和spark3上即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/local/zk root@spark2:/usr/local/</span><br><span class="line">scp -r /usr/local/zk root@spark3:/usr/local/</span><br><span class="line">scp ~/.bashrc root@spark2:~/</span><br><span class="line">scp ~/.bashrc root@spark3:~/</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>唯一的区别是spark2和spark3的标识号分别设置为1和2。</p>
<h3 id="启动ZooKeeper集群"><a href="#启动ZooKeeper集群" class="headerlink" title="启动ZooKeeper集群"></a>启动ZooKeeper集群</h3><p>分别在三台机器上执行：zkServer.sh start。<br>检查ZooKeeper状态：zkServer.sh status。</p>
<h2 id="kafka-2-9-2-0-8-1集群搭建"><a href="#kafka-2-9-2-0-8-1集群搭建" class="headerlink" title="kafka_2.9.2-0.8.1集群搭建"></a>kafka_2.9.2-0.8.1集群搭建</h2><h3 id="安装scala-2-11-4"><a href="#安装scala-2-11-4" class="headerlink" title="安装scala 2.11.4"></a>安装scala 2.11.4</h3><p>将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到spark1的&#x2F;usr&#x2F;local目录下。<br>对scala-2.11.4.tgz进行解压缩：tar -zxvf scala-2.11.4.tgz。<br>对scala目录进行重命名：mv scala-2.11.4 scala。</p>
<h3 id="配置scala相关的环境变量。"><a href="#配置scala相关的环境变量。" class="headerlink" title="配置scala相关的环境变量。"></a>配置scala相关的环境变量。</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi .bashrc</span><br><span class="line">export SCALA_HOME=/usr/local/scala</span><br><span class="line">export PATH=$SCALA_HOME/bin</span><br><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>
<p>查看scala是否安装成功：scala -version<br>按照上述步骤在spark2和spark3机器上都安装好scala。使用scp将scala和.bashrc拷贝到spark2和spark3上即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/local/scala root@spark2:/usr/local/</span><br><span class="line">scp -r /usr/local/scala root@spark3:/usr/local/</span><br><span class="line">scp ~/.bashrc root@spark2:~/</span><br><span class="line">scp ~/.bashrc root@spark3:~/</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<h3 id="安装Kafka包"><a href="#安装Kafka包" class="headerlink" title="安装Kafka包"></a>安装Kafka包</h3><p>将课程提供的kafka_2.9.2-0.8.1.tgz使用WinSCP拷贝到spark1的&#x2F;usr&#x2F;local目录下。<br>对kafka_2.9.2-0.8.1.tgz进行解压缩：tar -zxvf kafka_2.9.2-0.8.1.tgz。<br>对kafka目录进行改名：mv kafka_2.9.2-0.8.1 kafka。</p>
<h3 id="配置kafka。"><a href="#配置kafka。" class="headerlink" title="配置kafka。"></a>配置kafka。</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /usr/local/kafka/config/server.properties</span><br></pre></td></tr></table></figure>
<p>broker.id：依次增长的整数，0、1、2、3、4，集群中Broker的唯一id<br>zookeeper.connect&#x3D;192.168.1.107:2181,192.168.1.108:2181,192.168.1.109:2181<br>安装slf4j。<br>将课程提供的slf4j-1.7.6.zip上传到&#x2F;usr&#x2F;local目录下。unzip slf4j-1.7.6.zip。<br>把slf4j中的slf4j-nop-1.7.6.jar复制到kafka的libs目录下面。</p>
<h3 id="搭建kafka集群"><a href="#搭建kafka集群" class="headerlink" title="搭建kafka集群"></a>搭建kafka集群</h3><p>按照上述步骤在spark2和spark3分别安装kafka。用scp把kafka拷贝到spark2和spark3行即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/local/kafka root@spark2:/usr/local/</span><br><span class="line">scp -r /usr/local/kafka root@spark3:/usr/local/</span><br></pre></td></tr></table></figure>
<p>唯一区别的，就是server.properties中的broker.id，要设置为1和2。</p>
<h3 id="启动kafka集群"><a href="#启动kafka集群" class="headerlink" title="启动kafka集群"></a>启动kafka集群</h3><p>在三台机器上分别执行以下命令：nohup bin&#x2F;kafka-server-start.sh config&#x2F;server.properties &amp;。</p>
<p>解决kafka Unrecognized VM option ‘UseCompressedOops’问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi bin/kafka-run-class.sh </span><br><span class="line">if [ -z &quot;$KAFKA_JVM_PERFORMANCE_OPTS&quot; ]; then</span><br><span class="line">  KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server  -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<p>去掉-XX:+UseCompressedOops即可<br>使用jps检查启动是否成功。</p>
<h3 id="测试kafka集群"><a href="#测试kafka集群" class="headerlink" title="测试kafka集群"></a>测试kafka集群</h3><p>使用基本命令检查kafka是否搭建成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper spark1:2181,spark2:2181,spark3:2181 --topic Test --replication-factor 1 --partitions 1 --create</span><br><span class="line"></span><br><span class="line">bin/kafka-console-producer.sh --broker-list spark1:9092,spark2:9092,spark3:9092 --topic Test</span><br><span class="line"></span><br><span class="line">bin/kafka-console-consumer.sh --zookeeper spark1:2181,spark2:2181,spark3:2181 --topic Test --from-beginning</span><br></pre></td></tr></table></figure>

<h2 id="Spark-1-3-0集群搭建"><a href="#Spark-1-3-0集群搭建" class="headerlink" title="Spark 1.3.0集群搭建"></a>Spark 1.3.0集群搭建</h2><h2 id="安装spark包"><a href="#安装spark包" class="headerlink" title="安装spark包"></a>安装spark包</h2><p>将spark-1.3.0-bin-hadoop2.4.tgz使用WinSCP上传到&#x2F;usr&#x2F;local目录下。<br>解压缩spark包：tar zxvf spark-1.3.0-bin-hadoop2.4.tgz。<br>更改spark目录名：mv spark-1.3.0-bin-hadoop2.4 spark。<br>设置spark环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi .bashrc</span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$SPARK_HOME/bin</span><br><span class="line">export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</span><br><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>
<h3 id="修改spark-env-sh文件"><a href="#修改spark-env-sh文件" class="headerlink" title="修改spark-env.sh文件"></a>修改spark-env.sh文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf</span><br><span class="line">cp spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/latest</span><br><span class="line">export SCALA_HOME=/usr/local/scala</span><br><span class="line">export SPARK_MASTER_IP=192.168.1.107</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>
<h3 id="修改slaves文件-1"><a href="#修改slaves文件-1" class="headerlink" title="修改slaves文件"></a>修改slaves文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp slaves.template slaves</span><br><span class="line">vi slaves</span><br><span class="line">spark2</span><br><span class="line">spark3</span><br></pre></td></tr></table></figure>
<h3 id="安装spark集群"><a href="#安装spark集群" class="headerlink" title="安装spark集群"></a>安装spark集群</h3><p>在另外两个节点进行一模一样的配置，使用scp将spark和.bashrc拷贝到spark2和spark3即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/local/spark root@spark2:/usr/local/</span><br><span class="line">scp -r /usr/local/spark root@spark3:/usr/local/</span><br><span class="line">scp ~/.bashrc root@spark2:~/</span><br><span class="line">scp ~/.bashrc root@spark3:~/</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<h3 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h3><p>在spark目录下的sbin目录<br>执行，&#x2F;start-all.sh<br>使用jsp和8080端口可以检查集群是否启动成功<br>进入spark-shell查看是否正常</p>
<h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h2><h3 id="Java开发wordcount程序"><a href="#Java开发wordcount程序" class="headerlink" title="Java开发wordcount程序"></a>Java开发wordcount程序</h3><h3 id="安装eclipse"><a href="#安装eclipse" class="headerlink" title="安装eclipse"></a>安装eclipse</h3><p><a target="_blank" rel="noopener" href="https://www.eclipse.org/downloads/">eclipse download</a></p>
<h3 id="配置maven"><a href="#配置maven" class="headerlink" title="配置maven"></a>配置maven</h3><p>打开eclipse-&gt;新建Maven Project-&gt;maven-archetype-quickstart-&gt;groupId:cn.spark-&gt;artifactId:spark-study-java-&gt;package:cn.spark.study-&gt;替换pom.xml-quick fix(第一次要等待很长时间)-&gt;Buid Baph-&gt;Configure Build Path-&gt;Libaries-&gt;JRE System Libarary-&gt;Edit-&gt;Workspace default JRE。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">//: pom.xml</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;cn.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-study-java&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;spark-study-java&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.8.1&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">	  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">	  &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;</span><br><span class="line">	  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">	  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">	  &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;</span><br><span class="line">	  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">	  &lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">	  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">	  &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;</span><br><span class="line">	  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">	  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">	  &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;</span><br><span class="line">	  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">	  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">	  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">	  &lt;version&gt;2.4.1&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">	  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">	  &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;</span><br><span class="line">	  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;build&gt;</span><br><span class="line">    &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt;</span><br><span class="line">    &lt;testSourceDirectory&gt;src/main/test&lt;/testSourceDirectory&gt;</span><br><span class="line">	</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;descriptorRefs&gt;</span><br><span class="line">            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class="line">          &lt;/descriptorRefs&gt;</span><br><span class="line">          &lt;archive&gt;</span><br><span class="line">            &lt;manifest&gt;</span><br><span class="line">              &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">            &lt;/manifest&gt;</span><br><span class="line">          &lt;/archive&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">        &lt;executions&gt;</span><br><span class="line">          &lt;execution&gt;</span><br><span class="line">            &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">              &lt;goal&gt;single&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">          &lt;/execution&gt;</span><br><span class="line">        &lt;/executions&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.2.1&lt;/version&gt;</span><br><span class="line">        &lt;executions&gt;</span><br><span class="line">          &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">              &lt;goal&gt;exec&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">          &lt;/execution&gt;</span><br><span class="line">        &lt;/executions&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;executable&gt;java&lt;/executable&gt;</span><br><span class="line">          &lt;includeProjectDependencies&gt;true&lt;/includeProjectDependencies&gt;</span><br><span class="line">          &lt;includePluginDependencies&gt;false&lt;/includePluginDependencies&gt;</span><br><span class="line">          &lt;classpathScope&gt;compile&lt;/classpathScope&gt;</span><br><span class="line">          &lt;mainClass&gt;cn.spark.study.App&lt;/mainClass&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;source&gt;1.6&lt;/source&gt;</span><br><span class="line">          &lt;target&gt;1.6&lt;/target&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">  &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>

<h3 id="新建spark-txt，新建包cn-spark-study-core，新建WordCountLocal-java"><a href="#新建spark-txt，新建包cn-spark-study-core，新建WordCountLocal-java" class="headerlink" title="新建spark.txt，新建包cn.spark.study.core，新建WordCountLocal.java"></a>新建spark.txt，新建包cn.spark.study.core，新建WordCountLocal.java</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">//: spark.txt</span><br><span class="line">Spark is new technology that sits on top of Hadoop Distributed File System (HDFS) that is characterized as ¡°a fast and general engine for large-scale data processing.¡± Spark has three key features that make it the most interesting up and coming technology to rock the big data world since Apache Hadoop in 2005.</span><br><span class="line"></span><br><span class="line">1. For iterative analysis like logistic regression, Random Forests, or other advanced algorithms, Spark has demonstrated 100X increase in speed that scales to hundreds of millions of rows.</span><br><span class="line"></span><br><span class="line">2. Spark has native support for the latest and greatest programming languages Java, Scala, and of course Python.</span><br><span class="line"></span><br><span class="line">3. Spark has generality or platform compatibility in both directions meaning it integrates nicely with SQL engines (Shark), Machine Learning (MLlib), and streaming (Spark Streaming) without requiring new software installed on the cluster using Hadoop¡¯s new YARN cluster manager.</span><br><span class="line"></span><br><span class="line">At Alpine, we have made it dead simple to get started with Spark by including the technology in our latest build out of the box. We require no additional software or hardware to leverage our extensive list of operators for data transformation, exploration, and building advanced analytic models. We leverage Hadoop Yarn (Hadoop NextGen) to launch Spark job without any pre-installation of Spark or modification of cluster configuration. This empowers our customers to have seamless integration of our Spark implementation and their Hadoop stack. For example, we have analyzed 50 Million rows of account data in 50 seconds on a 20 node cluster recently at last month GigaOM conference.</span><br><span class="line"></span><br><span class="line">As a Spark certified company, Alpine Data Labs will be at the Summit. We¡¯d love to see you there!</span><br><span class="line"></span><br><span class="line">Want to meet with us?  Click here to set up an appointment at your convenience. Or just send a tweet to our Product &amp; Marketing Director Joel Horwitz @JSHorwitz.</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br></pre></td><td class="code"><pre><span class="line">//: WordCountLocal.java</span><br><span class="line">package cn.spark.study.core;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 使用java开发本地测试wordcount程序</span><br><span class="line"> * @author xdx</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class WordCountLocal &#123;</span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		// 第一步：创建SparkConf对象，设置Spark应用的配置信息</span><br><span class="line">		// 使用setMaster()可以设置Spark应用程序要连接的Spark集群的master节点的url</span><br><span class="line">		// 但是如果设置为local则代表，在本地运行</span><br><span class="line">		SparkConf conf = new SparkConf()</span><br><span class="line">				.setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">				.setMaster(&quot;local&quot;);</span><br><span class="line">		</span><br><span class="line">		// 第二步：创建JavaSparkContext对象</span><br><span class="line">		// 在Spark中，SparkContext是Spark所有功能的一个入口，你无论是用java、scala，甚至是python编写</span><br><span class="line">			// 都必须要有一个SparkContext，它的主要作用，包括初始化Spark应用程序所需的一些核心组件，包括</span><br><span class="line">			// 调度器（DAGSchedule、TaskScheduler），还会去到Spark Master节点上进行注册，等等</span><br><span class="line">		// 一句话，SparkContext，是Spark应用中，可以说是最最重要的一个对象</span><br><span class="line">		// 但是呢，在Spark中，编写不同类型的Spark应用程序，使用的SparkContext是不同的，如果使用scala，</span><br><span class="line">			// 使用的就是原生的SparkContext对象</span><br><span class="line">			// 但是如果使用Java，那么就是JavaSparkContext对象</span><br><span class="line">			// 如果是开发Spark SQL程序，那么就是SQLContext、HiveContext</span><br><span class="line">			// 如果是开发Spark Streaming程序，那么就是它独有的SparkContext</span><br><span class="line">			// 以此类推</span><br><span class="line">		JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">		</span><br><span class="line">		// 第三步：要针对输入源（hdfs文件、本地文件，等等），创建一个初始的RDD</span><br><span class="line">		// 输入源中的数据会打散，分配到RDD的每个partition中，从而形成一个初始的分布式的数据集</span><br><span class="line">		// 我们这里呢，因为是本地测试，所以呢，就是针对本地文件</span><br><span class="line">		// SparkContext中，用于根据文件类型的输入源创建RDD的方法，叫做textFile()方法</span><br><span class="line">		// 在Java中，创建的普通RDD，都叫做JavaRDD</span><br><span class="line">		// 在这里呢，RDD中，有元素这种概念，如果是hdfs或者本地文件呢，创建的RDD，每一个元素就相当于</span><br><span class="line">		// 是文件里的一行</span><br><span class="line">		JavaRDD&lt;String&gt; lines = sc.textFile(&quot;spark.txt&quot;);</span><br><span class="line">		</span><br><span class="line">		// 第四步：对初始RDD进行transformation操作，也就是一些计算操作</span><br><span class="line">		// 通常操作会通过创建function，并配合RDD的map、flatMap等算子来执行</span><br><span class="line">		// function，通常，如果比较简单，则创建指定Function的匿名内部类</span><br><span class="line">		// 但是如果function比较复杂，则会单独创建一个类，作为实现这个function接口的类</span><br><span class="line">		</span><br><span class="line">		// 先将每一行拆分成单个的单词</span><br><span class="line">		// FlatMapFunction，有两个泛型参数，分别代表了输入和输出类型</span><br><span class="line">		// 我们这里呢，输入肯定是String，因为是一行一行的文本，输出，其实也是String，因为是每一行的文本</span><br><span class="line">		// 这里先简要介绍flatMap算子的作用，其实就是，将RDD的一个元素，给拆分成一个或多个元素</span><br><span class="line">		JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public Iterable&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">				return Arrays.asList(line.split(&quot; &quot;));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		// 接着，需要将每一个单词，映射为(单词, 1)的这种格式</span><br><span class="line">			// 因为只有这样，后面才能根据单词作为key，来进行每个单词的出现次数的累加</span><br><span class="line">		// mapToPair，其实就是将每个元素，映射为一个(v1,v2)这样的Tuple2类型的元素</span><br><span class="line">			// 如果大家还记得scala里面讲的tuple，那么没错，这里的tuple2就是scala类型，包含了两个值</span><br><span class="line">		// mapToPair这个算子，要求的是与PairFunction配合使用，第一个泛型参数代表了输入类型</span><br><span class="line">			// 第二个和第三个泛型参数，代表的输出的Tuple2的第一个值和第二个值的类型</span><br><span class="line">		// JavaPairRDD的两个泛型参数，分别代表了tuple元素的第一个值和第二个值的类型</span><br><span class="line">		JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">				return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		// 接着，需要以单词作为key，统计每个单词出现的次数</span><br><span class="line">		// 这里要使用reduceByKey这个算子，对每个key对应的value，都进行reduce操作</span><br><span class="line">		// 比如JavaPairRDD中有几个元素，分别为(hello, 1) (hello, 1) (hello, 1) (world, 1)</span><br><span class="line">		// reduce操作，相当于是把第一个值和第二个值进行计算，然后再将结果与第三个值进行计算</span><br><span class="line">		// 比如这里的hello，那么就相当于是，首先是1 + 1 = 2，然后再将2 + 1 = 3</span><br><span class="line">		// 最后返回的JavaPairRDD中的元素，也是tuple，但是第一个值就是每个key，第二个值就是key的value</span><br><span class="line">		// reduce之后的结果，相当于就是每个单词出现的次数</span><br><span class="line">		JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">				return v1 + v2;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		// 到这里为止，我们通过几个Spark算子操作，已经统计出了单词的次数</span><br><span class="line">		// 但是，之前我们使用的flatMap、mapToPair、reduceByKey这种操作，都叫做transformation操作</span><br><span class="line">		// 一个Spark应用中，光是有transformation操作，是不行的，是不会执行的，必须要有一种叫做action</span><br><span class="line">		// 接着，最后，可以使用一种叫做action操作的，比如说，foreach，来触发程序的执行</span><br><span class="line">		wordCounts.foreach(new VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public void call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123;</span><br><span class="line">				System.out.println(wordCount._1 + &quot; appeared &quot; + wordCount._2 + &quot; times.&quot;);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		sc.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">/* Output:</span><br><span class="line">Hadoop appeared 1 times.</span><br><span class="line">processing. appeared 1 times.</span><br><span class="line">Spark appeared 6 times.</span><br><span class="line">it appeared 2 times.</span><br><span class="line">parallel appeared 1 times.</span><br><span class="line">its appeared 1 times.</span><br><span class="line">operators appeared 1 times.</span><br><span class="line">YARN, appeared 1 times.</span><br><span class="line">engine. appeared 1 times.</span><br><span class="line">Runs appeared 1 times.</span><br><span class="line">standalone appeared 1 times.</span><br><span class="line">optimizer, appeared 1 times.</span><br><span class="line">shells. appeared 1 times.</span><br><span class="line">complex appeared 1 times.</span><br><span class="line">state-of-the-art appeared 1 times.</span><br><span class="line">learning, appeared 1 times.</span><br><span class="line">Use appeared 1 times.</span><br><span class="line">applications appeared 1 times.</span><br><span class="line">over appeared 1 times.</span><br><span class="line">streaming, appeared 1 times.</span><br><span class="line">easy appeared 1 times.</span><br><span class="line">for appeared 3 times.</span><br><span class="line">faster. appeared 1 times.</span><br><span class="line">make appeared 1 times.</span><br><span class="line">engine appeared 1 times.</span><br><span class="line">these appeared 1 times.</span><br><span class="line">performance appeared 1 times.</span><br><span class="line">the appeared 3 times.</span><br><span class="line">application. appeared 1 times.</span><br><span class="line">SQL. appeared 1 times.</span><br><span class="line">DataFrames, appeared 1 times.</span><br><span class="line">Mesos, appeared 2 times.</span><br><span class="line">R, appeared 2 times.</span><br><span class="line">can appeared 4 times.</span><br><span class="line">HDFS, appeared 1 times.</span><br><span class="line">build appeared 1 times.</span><br><span class="line">Cassandra, appeared 1 times.</span><br><span class="line">achieves appeared 1 times.</span><br><span class="line">Apache appeared 6 times.</span><br><span class="line">including appeared 1 times.</span><br><span class="line">large-scale appeared 1 times.</span><br><span class="line">Kubernetes, appeared 1 times.</span><br><span class="line">sources. appeared 2 times.</span><br><span class="line">analytics. appeared 1 times.</span><br><span class="line">libraries appeared 2 times.</span><br><span class="line">Combine appeared 1 times.</span><br><span class="line">query appeared 1 times.</span><br><span class="line">batch appeared 1 times.</span><br><span class="line">It appeared 1 times.</span><br><span class="line">scheduler, appeared 1 times.</span><br><span class="line">both appeared 1 times.</span><br><span class="line">streaming appeared 1 times.</span><br><span class="line">Access appeared 1 times.</span><br><span class="line">machine appeared 1 times.</span><br><span class="line">Everywhere appeared 1 times.</span><br><span class="line">Generality appeared 1 times.</span><br><span class="line">stack appeared 1 times.</span><br><span class="line">And appeared 1 times.</span><br><span class="line">high appeared 1 times.</span><br><span class="line">Speed appeared 1 times.</span><br><span class="line">is appeared 1 times.</span><br><span class="line">80 appeared 1 times.</span><br><span class="line">run appeared 1 times.</span><br><span class="line">seamlessly appeared 1 times.</span><br><span class="line">Kubernetes. appeared 1 times.</span><br><span class="line">Spark™ appeared 1 times.</span><br><span class="line">runs appeared 1 times.</span><br><span class="line">same appeared 1 times.</span><br><span class="line">You appeared 2 times.</span><br><span class="line">on appeared 5 times.</span><br><span class="line">interactively appeared 1 times.</span><br><span class="line">Ease appeared 1 times.</span><br><span class="line">data appeared 4 times.</span><br><span class="line">apps. appeared 1 times.</span><br><span class="line">offers appeared 1 times.</span><br><span class="line">in appeared 4 times.</span><br><span class="line">using appeared 2 times.</span><br><span class="line">DAG appeared 1 times.</span><br><span class="line">Alluxio, appeared 1 times.</span><br><span class="line">diverse appeared 1 times.</span><br><span class="line">100x appeared 1 times.</span><br><span class="line">execution appeared 1 times.</span><br><span class="line">hundreds appeared 1 times.</span><br><span class="line">Python, appeared 2 times.</span><br><span class="line">from appeared 1 times.</span><br><span class="line">other appeared 1 times.</span><br><span class="line">standalone, appeared 1 times.</span><br><span class="line">use appeared 1 times.</span><br><span class="line">physical appeared 1 times.</span><br><span class="line">workloads appeared 1 times.</span><br><span class="line">Run appeared 1 times.</span><br><span class="line">mode, appeared 1 times.</span><br><span class="line">EC2, appeared 1 times.</span><br><span class="line">you appeared 1 times.</span><br><span class="line">that appeared 1 times.</span><br><span class="line">or appeared 2 times.</span><br><span class="line">a appeared 5 times.</span><br><span class="line">data, appeared 1 times.</span><br><span class="line">high-level appeared 1 times.</span><br><span class="line">Java, appeared 1 times.</span><br><span class="line">SQL appeared 2 times.</span><br><span class="line">Hive, appeared 1 times.</span><br><span class="line">Hadoop, appeared 1 times.</span><br><span class="line">to appeared 1 times.</span><br><span class="line"> appeared 9 times.</span><br><span class="line">analytics appeared 1 times.</span><br><span class="line">GraphX, appeared 1 times.</span><br><span class="line">Write appeared 1 times.</span><br><span class="line">of appeared 3 times.</span><br><span class="line">cluster appeared 1 times.</span><br><span class="line">access appeared 1 times.</span><br><span class="line">MLlib appeared 1 times.</span><br><span class="line">quickly appeared 1 times.</span><br><span class="line">Scala, appeared 2 times.</span><br><span class="line">HBase, appeared 1 times.</span><br><span class="line">and appeared 8 times.</span><br><span class="line">unified appeared 1 times.</span><br><span class="line">SQL, appeared 1 times.</span><br><span class="line">combine appeared 1 times.</span><br><span class="line">Streaming. appeared 1 times.</span><br><span class="line">powers appeared 1 times.</span><br><span class="line">cloud. appeared 1 times.</span><br><span class="line">*///:~</span><br></pre></td></tr></table></figure>
<h3 id="spark-submit提交到spark集群进行执行"><a href="#spark-submit提交到spark集群进行执行" class="headerlink" title="spark-submit提交到spark集群进行执行"></a>spark-submit提交到spark集群进行执行</h3><p>编写WordCountCluster.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">//: WordCountCluster.java</span><br><span class="line">package cn.spark.study.core;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 将java开发的wordcount程序部署到spark集群上运行</span><br><span class="line"> * @author xdx</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class WordCountCluster &#123;</span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		// 如果要在spark集群上运行，需要修改的，只有两个地方</span><br><span class="line">		// 第一，将SparkConf的setMaster()方法给删掉，默认它自己会去连接</span><br><span class="line">		// 第二，我们针对的不是本地文件了，修改为hadoop hdfs上的真正的存储大数据的文件</span><br><span class="line">		</span><br><span class="line">		// 实际执行步骤：</span><br><span class="line">		// 1、将spark.txt文件上传到hdfs上去</span><br><span class="line">		// 2、使用我们最早在pom.xml里配置的maven插件，对spark工程进行打包</span><br><span class="line">		// 3、将打包后的spark工程jar包，上传到机器上执行</span><br><span class="line">		// 4、编写spark-submit脚本</span><br><span class="line">		// 5、执行spark-submit脚本，提交spark应用到集群执行</span><br><span class="line">		</span><br><span class="line">		SparkConf conf = new SparkConf()</span><br><span class="line">				.setAppName(&quot;WordCountCluster&quot;);</span><br><span class="line">		</span><br><span class="line">		JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">		</span><br><span class="line">		JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://spark1:9000/spark.txt&quot;);</span><br><span class="line">		</span><br><span class="line">		JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public Iterable&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">				return Arrays.asList(line.split(&quot; &quot;));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">				return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override </span><br><span class="line">			public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">				return v1 + v2;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		wordCounts.foreach(new VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">			private static final long serialVersionID = 1L;</span><br><span class="line">			</span><br><span class="line">			@Override</span><br><span class="line">			public void call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123;</span><br><span class="line">				System.out.println(wordCount._1 + &quot; appeared &quot; + wordCount._2 + &quot; times.&quot;);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">		sc.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">/* Output:</span><br><span class="line">*///:~</span><br></pre></td></tr></table></figure>

<p>将spark.txt上传到spark1，将spark.txt上传到hdfs<br>将spark-study-java打包，Run As-&gt;Run Configurations-&gt;Maven Build-&gt;New-&gt;spark-study-java-&gt;Run<br>将&#x2F;spark-study-java&#x2F;target&#x2F;spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar上传到spark<br>编写wordcount.sh，使用spark-submit进行执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//: wordcount.sh</span><br><span class="line">/usr/local/spark/bin/spark-submit \</span><br><span class="line">--class cn.spark.sparktest.core.WordCountCluster \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--driver-memory 100m \</span><br><span class="line">--executor-memory 100m \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/root/Workspace/SparkProjects/wordcount/SparkTest-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</span><br></pre></td></tr></table></figure>

<h3 id="Scala开发wordcount程序"><a href="#Scala开发wordcount程序" class="headerlink" title="Scala开发wordcount程序"></a>Scala开发wordcount程序</h3><p>scala ide for eclipse download<br>在Java Build Path中，添加spark依赖包(spark-assembly-1.3.0-hadoop2.4.0.jar)，如果与scala ide for eclipse原生的scala版本发生冲突，则移除原生的scala，重新配置scala compiler</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br></pre></td><td class="code"><pre><span class="line">//: WordCount.scala</span><br><span class="line">package cn.spark.study.core</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @author xdx</span><br><span class="line"> */</span><br><span class="line">object WordCount &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;WordCount&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val lines = sc.textFile(&quot;spark.txt&quot;, 1)</span><br><span class="line">    val words = lines.flatMap &#123; line =&gt; line.split(&quot; &quot;) &#125;</span><br><span class="line">    val pairs = words.map &#123; word =&gt; (word, 1) &#125;</span><br><span class="line">    val wordCounts = pairs.reduceByKey &#123; _ + _ &#125;</span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot; appeared &quot; + wordCount._2 + &quot; times.&quot;))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">/* Output:</span><br><span class="line">(MLlib), appeared 1 times.</span><br><span class="line">For appeared 2 times.</span><br><span class="line">Product appeared 1 times.</span><br><span class="line">it appeared 3 times.</span><br><span class="line">operators appeared 1 times.</span><br><span class="line">sits appeared 1 times.</span><br><span class="line">Hadoop��s appeared 1 times.</span><br><span class="line">have appeared 3 times.</span><br><span class="line">tweet appeared 1 times.</span><br><span class="line">stack. appeared 1 times.</span><br><span class="line">modification appeared 1 times.</span><br><span class="line">conference. appeared 1 times.</span><br><span class="line">we appeared 2 times.</span><br><span class="line">requiring appeared 1 times.</span><br><span class="line">This appeared 1 times.</span><br><span class="line">simple appeared 1 times.</span><br><span class="line">manager. appeared 1 times.</span><br><span class="line">software appeared 2 times.</span><br><span class="line">any appeared 1 times.</span><br><span class="line">make appeared 1 times.</span><br><span class="line">implementation appeared 1 times.</span><br><span class="line">seconds appeared 1 times.</span><br><span class="line">&amp; appeared 1 times.</span><br><span class="line">out appeared 1 times.</span><br><span class="line">Data appeared 1 times.</span><br><span class="line">engine appeared 1 times.</span><br><span class="line">directions appeared 1 times.</span><br><span class="line">month appeared 1 times.</span><br><span class="line">the appeared 7 times.</span><br><span class="line">technology appeared 3 times.</span><br><span class="line">2. appeared 1 times.</span><br><span class="line">Alpine, appeared 1 times.</span><br><span class="line">We��d appeared 1 times.</span><br><span class="line">box. appeared 1 times.</span><br><span class="line">100X appeared 1 times.</span><br><span class="line">most appeared 1 times.</span><br><span class="line">build appeared 1 times.</span><br><span class="line">love appeared 1 times.</span><br><span class="line">be appeared 1 times.</span><br><span class="line">��a appeared 1 times.</span><br><span class="line">Apache appeared 1 times.</span><br><span class="line">At appeared 1 times.</span><br><span class="line">Alpine appeared 1 times.</span><br><span class="line">our appeared 5 times.</span><br><span class="line">including appeared 1 times.</span><br><span class="line">as appeared 1 times.</span><br><span class="line">us? appeared 1 times.</span><br><span class="line">dead appeared 1 times.</span><br><span class="line">iterative appeared 1 times.</span><br><span class="line">leverage appeared 2 times.</span><br><span class="line">Want appeared 1 times.</span><br><span class="line">File appeared 1 times.</span><br><span class="line">programming appeared 1 times.</span><br><span class="line">account appeared 1 times.</span><br><span class="line">recently appeared 1 times.</span><br><span class="line">engines appeared 1 times.</span><br><span class="line">is appeared 2 times.</span><br><span class="line">Horwitz appeared 1 times.</span><br><span class="line">on appeared 3 times.</span><br><span class="line">features appeared 1 times.</span><br><span class="line">pre-installation appeared 1 times.</span><br><span class="line">speed appeared 1 times.</span><br><span class="line">at appeared 3 times.</span><br><span class="line">using appeared 1 times.</span><br><span class="line">convenience. appeared 1 times.</span><br><span class="line">top appeared 1 times.</span><br><span class="line">integrates appeared 1 times.</span><br><span class="line">meaning appeared 1 times.</span><br><span class="line">customers appeared 1 times.</span><br><span class="line">new appeared 3 times.</span><br><span class="line">We appeared 2 times.</span><br><span class="line">Python. appeared 1 times.</span><br><span class="line">Random appeared 1 times.</span><br><span class="line">launch appeared 1 times.</span><br><span class="line">processing.�� appeared 1 times.</span><br><span class="line">set appeared 1 times.</span><br><span class="line">has appeared 4 times.</span><br><span class="line">NextGen) appeared 1 times.</span><br><span class="line">world appeared 1 times.</span><br><span class="line">Learning appeared 1 times.</span><br><span class="line">seamless appeared 1 times.</span><br><span class="line">Director appeared 1 times.</span><br><span class="line">generality appeared 1 times.</span><br><span class="line">or appeared 4 times.</span><br><span class="line">Yarn appeared 1 times.</span><br><span class="line">Java, appeared 1 times.</span><br><span class="line">appointment appeared 1 times.</span><br><span class="line">As appeared 1 times.</span><br><span class="line">YARN appeared 1 times.</span><br><span class="line">Machine appeared 1 times.</span><br><span class="line">company, appeared 1 times.</span><br><span class="line">installed appeared 1 times.</span><br><span class="line">50 appeared 2 times.</span><br><span class="line">see appeared 1 times.</span><br><span class="line">of appeared 10 times.</span><br><span class="line">cluster appeared 4 times.</span><br><span class="line">three appeared 1 times.</span><br><span class="line">analytic appeared 1 times.</span><br><span class="line">Or appeared 1 times.</span><br><span class="line">Forests, appeared 1 times.</span><br><span class="line">rows appeared 1 times.</span><br><span class="line">millions appeared 1 times.</span><br><span class="line">rows. appeared 1 times.</span><br><span class="line">Hadoop appeared 4 times.</span><br><span class="line">characterized appeared 1 times.</span><br><span class="line">Spark appeared 10 times.</span><br><span class="line">integration appeared 1 times.</span><br><span class="line">job appeared 1 times.</span><br><span class="line">native appeared 1 times.</span><br><span class="line">greatest appeared 1 times.</span><br><span class="line">general appeared 1 times.</span><br><span class="line">Million appeared 1 times.</span><br><span class="line">extensive appeared 1 times.</span><br><span class="line">here appeared 1 times.</span><br><span class="line">big appeared 1 times.</span><br><span class="line">Joel appeared 1 times.</span><br><span class="line">1. appeared 1 times.</span><br><span class="line">send appeared 1 times.</span><br><span class="line">(HDFS) appeared 1 times.</span><br><span class="line">3. appeared 1 times.</span><br><span class="line">without appeared 2 times.</span><br><span class="line">for appeared 3 times.</span><br><span class="line">models. appeared 1 times.</span><br><span class="line">require appeared 1 times.</span><br><span class="line">just appeared 1 times.</span><br><span class="line">@JSHorwitz. appeared 1 times.</span><br><span class="line">Labs appeared 1 times.</span><br><span class="line">latest appeared 2 times.</span><br><span class="line">regression, appeared 1 times.</span><br><span class="line">node appeared 1 times.</span><br><span class="line">coming appeared 1 times.</span><br><span class="line">your appeared 1 times.</span><br><span class="line">up appeared 2 times.</span><br><span class="line">analysis appeared 1 times.</span><br><span class="line">20 appeared 1 times.</span><br><span class="line">advanced appeared 2 times.</span><br><span class="line">Distributed appeared 1 times.</span><br><span class="line">no appeared 1 times.</span><br><span class="line">large-scale appeared 1 times.</span><br><span class="line">since appeared 1 times.</span><br><span class="line">started appeared 1 times.</span><br><span class="line">empowers appeared 1 times.</span><br><span class="line">transformation, appeared 1 times.</span><br><span class="line">by appeared 1 times.</span><br><span class="line">like appeared 1 times.</span><br><span class="line">compatibility appeared 1 times.</span><br><span class="line">2005. appeared 1 times.</span><br><span class="line">both appeared 1 times.</span><br><span class="line">an appeared 1 times.</span><br><span class="line">streaming appeared 1 times.</span><br><span class="line">(Shark), appeared 1 times.</span><br><span class="line">analyzed appeared 1 times.</span><br><span class="line">Streaming) appeared 1 times.</span><br><span class="line">made appeared 1 times.</span><br><span class="line">nicely appeared 1 times.</span><br><span class="line">configuration. appeared 1 times.</span><br><span class="line">with appeared 3 times.</span><br><span class="line">algorithms, appeared 1 times.</span><br><span class="line">meet appeared 1 times.</span><br><span class="line">data appeared 4 times.</span><br><span class="line">interesting appeared 1 times.</span><br><span class="line">in appeared 5 times.</span><br><span class="line">logistic appeared 1 times.</span><br><span class="line">GigaOM appeared 1 times.</span><br><span class="line">Summit. appeared 1 times.</span><br><span class="line">increase appeared 1 times.</span><br><span class="line">hundreds appeared 1 times.</span><br><span class="line">support appeared 1 times.</span><br><span class="line">scales appeared 1 times.</span><br><span class="line">Click appeared 1 times.</span><br><span class="line">building appeared 1 times.</span><br><span class="line">other appeared 1 times.</span><br><span class="line">course appeared 1 times.</span><br><span class="line">exploration, appeared 1 times.</span><br><span class="line">rock appeared 1 times.</span><br><span class="line">key appeared 1 times.</span><br><span class="line">you appeared 1 times.</span><br><span class="line">hardware appeared 1 times.</span><br><span class="line">that appeared 4 times.</span><br><span class="line">a appeared 3 times.</span><br><span class="line">fast appeared 1 times.</span><br><span class="line">their appeared 1 times.</span><br><span class="line">example, appeared 1 times.</span><br><span class="line">last appeared 1 times.</span><br><span class="line">SQL appeared 1 times.</span><br><span class="line">demonstrated appeared 1 times.</span><br><span class="line">will appeared 1 times.</span><br><span class="line">to appeared 10 times.</span><br><span class="line">get appeared 1 times.</span><br><span class="line">platform appeared 1 times.</span><br><span class="line"> appeared 7 times.</span><br><span class="line">languages appeared 1 times.</span><br><span class="line">list appeared 1 times.</span><br><span class="line">there! appeared 1 times.</span><br><span class="line">(Spark appeared 1 times.</span><br><span class="line">Scala, appeared 1 times.</span><br><span class="line">and appeared 7 times.</span><br><span class="line">Marketing appeared 1 times.</span><br><span class="line">(Hadoop appeared 1 times.</span><br><span class="line">certified appeared 1 times.</span><br><span class="line">additional appeared 1 times.</span><br><span class="line">System appeared 1 times.</span><br><span class="line">*///:~</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/13/Scala%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" rel="next" title="Scala快速入门">
                <i class="fa fa-chevron-left"></i> Scala快速入门
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/13/Spark%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" rel="prev" title="Spark运行环境搭建">
                Spark运行环境搭建 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiadx" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CentOS-6-5%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="nav-number">1.</span> <span class="nav-text">CentOS 6.5集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#VirtualBox%E5%AE%89%E8%A3%85"><span class="nav-number">1.1.</span> <span class="nav-text">VirtualBox安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CentOS-6-5%E5%AE%89%E8%A3%85"><span class="nav-number">1.2.</span> <span class="nav-text">CentOS 6.5安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CentOS-6-5%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE"><span class="nav-number">1.3.</span> <span class="nav-text">CentOS 6.5网络配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CentOS-6-5%E9%98%B2%E7%81%AB%E5%A2%99%E5%92%8CDNS%E9%85%8D%E7%BD%AE"><span class="nav-number">1.4.</span> <span class="nav-text">CentOS 6.5防火墙和DNS配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CentOS-6-5-yum%E9%85%8D%E7%BD%AE"><span class="nav-number">1.5.</span> <span class="nav-text">CentOS 6.5 yum配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JDK-1-7%E5%AE%89%E8%A3%85"><span class="nav-number">1.6.</span> <span class="nav-text">JDK 1.7安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E7%AC%AC%E4%BA%8C%E5%8F%B0%E5%92%8C%E7%AC%AC%E4%B8%89%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="nav-number">1.7.</span> <span class="nav-text">安装第二台和第三台虚拟机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4ssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95"><span class="nav-number">1.8.</span> <span class="nav-text">配置集群ssh免密码登录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop-2-4-1%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="nav-number">2.</span> <span class="nav-text">Hadoop 2.4.1集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85hadoop%E5%8C%85"><span class="nav-number">2.1.</span> <span class="nav-text">安装hadoop包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9core-site-xml"><span class="nav-number">2.2.</span> <span class="nav-text">修改core-site.xml</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9hdfs-site-xml"><span class="nav-number">2.3.</span> <span class="nav-text">修改hdfs-site.xml</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9slaves%E6%96%87%E4%BB%B6"><span class="nav-number">2.4.</span> <span class="nav-text">修改slaves文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E5%8F%A6%E5%A4%96%E4%B8%A4%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%90%AD%E5%BB%BAhadoop"><span class="nav-number">2.5.</span> <span class="nav-text">在另外两台机器上搭建hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8hdfs%E9%9B%86%E7%BE%A4"><span class="nav-number">2.6.</span> <span class="nav-text">启动hdfs集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8yarn%E9%9B%86%E7%BE%A4"><span class="nav-number">2.7.</span> <span class="nav-text">启动yarn集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-0-13%E6%90%AD%E5%BB%BA"><span class="nav-number">2.8.</span> <span class="nav-text">Hive 0.13搭建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85mysql"><span class="nav-number">2.9.</span> <span class="nav-text">安装mysql</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEhive-site-xml"><span class="nav-number">2.10.</span> <span class="nav-text">配置hive-site.xml</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEhive-env-sh%E5%92%8Chive-config-sh"><span class="nav-number">2.11.</span> <span class="nav-text">配置hive-env.sh和hive-config.sh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81hive%E6%98%AF%E5%90%A6%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F"><span class="nav-number">2.12.</span> <span class="nav-text">验证hive是否安装成功</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ZooKeeper-3-4-5%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="nav-number">3.</span> <span class="nav-text">ZooKeeper 3.4.5集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85ZooKeeper%E5%8C%85"><span class="nav-number">3.1.</span> <span class="nav-text">安装ZooKeeper包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEzoo-cfg"><span class="nav-number">3.2.</span> <span class="nav-text">配置zoo.cfg</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AEzk%E8%8A%82%E7%82%B9%E6%A0%87%E8%AF%86"><span class="nav-number">3.3.</span> <span class="nav-text">设置zk节点标识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%AD%E5%BB%BAzk%E9%9B%86%E7%BE%A4"><span class="nav-number">3.4.</span> <span class="nav-text">搭建zk集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8ZooKeeper%E9%9B%86%E7%BE%A4"><span class="nav-number">3.5.</span> <span class="nav-text">启动ZooKeeper集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-2-9-2-0-8-1%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="nav-number">4.</span> <span class="nav-text">kafka_2.9.2-0.8.1集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85scala-2-11-4"><span class="nav-number">4.1.</span> <span class="nav-text">安装scala 2.11.4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEscala%E7%9B%B8%E5%85%B3%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E3%80%82"><span class="nav-number">4.2.</span> <span class="nav-text">配置scala相关的环境变量。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Kafka%E5%8C%85"><span class="nav-number">4.3.</span> <span class="nav-text">安装Kafka包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEkafka%E3%80%82"><span class="nav-number">4.4.</span> <span class="nav-text">配置kafka。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%AD%E5%BB%BAkafka%E9%9B%86%E7%BE%A4"><span class="nav-number">4.5.</span> <span class="nav-text">搭建kafka集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8kafka%E9%9B%86%E7%BE%A4"><span class="nav-number">4.6.</span> <span class="nav-text">启动kafka集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95kafka%E9%9B%86%E7%BE%A4"><span class="nav-number">4.7.</span> <span class="nav-text">测试kafka集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-1-3-0%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="nav-number">5.</span> <span class="nav-text">Spark 1.3.0集群搭建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85spark%E5%8C%85"><span class="nav-number">6.</span> <span class="nav-text">安装spark包</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9spark-env-sh%E6%96%87%E4%BB%B6"><span class="nav-number">6.1.</span> <span class="nav-text">修改spark-env.sh文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9slaves%E6%96%87%E4%BB%B6-1"><span class="nav-number">6.2.</span> <span class="nav-text">修改slaves文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85spark%E9%9B%86%E7%BE%A4"><span class="nav-number">6.3.</span> <span class="nav-text">安装spark集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8spark%E9%9B%86%E7%BE%A4"><span class="nav-number">6.4.</span> <span class="nav-text">启动spark集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WordCount"><span class="nav-number">7.</span> <span class="nav-text">WordCount</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Java%E5%BC%80%E5%8F%91wordcount%E7%A8%8B%E5%BA%8F"><span class="nav-number">7.1.</span> <span class="nav-text">Java开发wordcount程序</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85eclipse"><span class="nav-number">7.2.</span> <span class="nav-text">安装eclipse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEmaven"><span class="nav-number">7.3.</span> <span class="nav-text">配置maven</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B0%E5%BB%BAspark-txt%EF%BC%8C%E6%96%B0%E5%BB%BA%E5%8C%85cn-spark-study-core%EF%BC%8C%E6%96%B0%E5%BB%BAWordCountLocal-java"><span class="nav-number">7.4.</span> <span class="nav-text">新建spark.txt，新建包cn.spark.study.core，新建WordCountLocal.java</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-submit%E6%8F%90%E4%BA%A4%E5%88%B0spark%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="nav-number">7.5.</span> <span class="nav-text">spark-submit提交到spark集群进行执行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scala%E5%BC%80%E5%8F%91wordcount%E7%A8%8B%E5%BA%8F"><span class="nav-number">7.6.</span> <span class="nav-text">Scala开发wordcount程序</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dingxin</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
